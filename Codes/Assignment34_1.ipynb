{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing - Assignment 3 & 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    regex_word = re.compile(r\"[a-zA-Z]+\")\n",
    "    regex_punt = re.compile(r\"[\\-\\,\\.;\\'\\`\\\"\\?!]\")\n",
    "    regex_email = re.compile(r\"[a-z0-9#$_\\-]+(?:\\.[a-z0-9#$_\\-]+)*@(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\",re.IGNORECASE)\n",
    "    regex_url = re.compile(r\"(http://)?(https://)?(ftp://)?(www\\.)?[^\\s/$.?#]*\\.[^\\s]+\")         \n",
    "    regex_currency = re.compile(r\"[$₹€£¥\\+\\-]*[0-9,.]+[$₹€£¥]*\")        \n",
    "    regex_name = re.compile(r\"[A-Z][a-z]+\")        \n",
    "    regex_hashtag = re.compile(r\"#[a-zA-Z_0-9$\\-]+\")        \n",
    "    regex_mention = re.compile(r\"@[a-zA-Z0-9$_\\-]+\") \n",
    "    # all_regex = re.compile(regex_punt '|' regex_word '|' regex_email '|' regex_url '|' regex_currency '|' regex_name '|' regex_hashtag '|' regex_mention)\n",
    "    if len(sys.argv)<2:\n",
    "        print(\"Error! Enter name of file to parse\")\n",
    "        print(\"Program will exit\")\n",
    "        sys.exit()\n",
    "    file_name =  sys.argv[1]\n",
    "    # print(\"Reading from file:\",file_name)\n",
    "    word_list = []\n",
    "    punc_list = []\n",
    "    email_list = []\n",
    "    url_list = []\n",
    "    currency_list = []\n",
    "    name_list = []\n",
    "    hashtag_list = []\n",
    "    mention_list = []\n",
    "    # tok_list = []\n",
    "    with open(file_name, 'r') as fh:\n",
    "        for line in fh:\n",
    "            # tok_list.append(all_regex.finditer(line))\n",
    "            tokenised_list = []\n",
    "            # iter = re.finditer(regex_word,line)\n",
    "            # if iter:\n",
    "            #     word_list.append(iter)\n",
    "            # else:\n",
    "            #     iter = re.finditer(regex_punt,line)\n",
    "            #     if iter:\n",
    "            #         punc_list.append\n",
    "            # iter = re.finditer(regex_word,line)\n",
    "            # if iter:\n",
    "            #     word_list.append(iter)\n",
    "            #     continue\n",
    "            hashtag_list.append(re.finditer(regex_hashtag,line))\n",
    "            mention_list.append(re.finditer(regex_mention,line))\n",
    "            word_list.append(re.finditer(regex_word,line))\n",
    "            punc_list.append(re.finditer(regex_punt,line))\n",
    "            email_list.append(re.finditer(regex_email,line))\n",
    "            url_list.append(re.finditer(regex_url,line))\n",
    "            currency_list.append(re.finditer(regex_currency,line))\n",
    "            name_list.append(re.finditer(regex_name,line))\n",
    "\n",
    "            for iters in word_list:\n",
    "                for match in iters:\n",
    "                    tokenised_list.append(match.span())\n",
    "            for iters in name_list:\n",
    "                for match in iters:\n",
    "                    tokenised_list.append(match.span())\n",
    "            for iters in hashtag_list:\n",
    "                for match in iters:\n",
    "                    tokenised_list.append(match.span())\n",
    "            for iters in mention_list:\n",
    "                for match in iters:\n",
    "                   tokenised_list.append(match.span())\n",
    "            for iters in punc_list:\n",
    "                for match in iters:\n",
    "                    tokenised_list.append(match.span()) \n",
    "            for iters in email_list:\n",
    "                for match in iters:\n",
    "                    tokenised_list.append(match.span())\n",
    "            for iters in url_list:\n",
    "                for match in iters:\n",
    "                    tokenised_list.append(match.span())\n",
    "            for iters in currency_list:\n",
    "                for match in iters:\n",
    "                    tokenised_list.append(match.span())\n",
    "            # for iters in tok_list:\n",
    "            #     for match in iters:\n",
    "            #         tokenised_list.append(match.span())\n",
    "            tokenised_list = list(set(tokenised_list))\n",
    "            tokenised_list.sort()\n",
    "            indexes = []\n",
    "            for tokens in tokenised_list:\n",
    "                start = tokens[0]\n",
    "                end = tokens[1]\n",
    "                if end not in indexes:\n",
    "                    indexes.append(start)\n",
    "                    indexes.append(end)\n",
    "\n",
    "            indexes.sort()\n",
    "            print(line,end='')\n",
    "            # print(indexes)\n",
    "            for i in range(0,len(indexes)-1,2):\n",
    "                # print(indexes[i],indexes[i+1])\n",
    "                if i+1==len(indexes):\n",
    "                    if line[indexes[i]]!='\\n':\n",
    "                        print(line[indexes[i]],sep='',end=' ')\n",
    "                else:\n",
    "                    print(line[indexes[i]:indexes[i+1]],sep='',end=' ')\n",
    "            print(\"\")\n",
    "            print(\"\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 12)\n",
      "Do not #hate me. I am just a @TA.\n",
      "[0, 2, 3, 6, 7, 8, 12, 12, 13, 15, 15, 16, 17, 18, 19, 21, 22, 26, 27, 28, 29, 30, 32, 32, 32, 33]\n",
      "0 2\n",
      "3 6\n",
      "7 8\n",
      "12 12\n",
      "13 15\n",
      "15 16\n",
      "17 18\n",
      "19 21\n",
      "22 26\n",
      "27 28\n",
      "29 30\n",
      "32 32\n",
      "32 33\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run tokenizer.py test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the shop where I bought my bike.\n",
      "This is the shop where I bought my bike . \n",
      "\n",
      "The driver drinks liquor, I think someone else ought to drive.\n",
      "The driver drinks liquor , I think someone else ought to drive . \n",
      "\n",
      "I would go with you, only I have no money.\n",
      "I would go with you , only I have no money . \n",
      "\n",
      "She studies grammar.\n",
      "She studies grammar . \n",
      "\n",
      "You will take money. The amount will be +100$ -2000₹; so that the total is ₹2100$\n",
      "You will take money . The amount will be +100$  2000₹ ; so that the total is ₹2100$ \n",
      "\n",
      "Was she writing a letter ?\n",
      "Was she writing a letter ? \n",
      "\n",
      "The peon opened the gate.\n",
      "The peon opened the gate . \n",
      "\n",
      "The police studied the stills from the security video.\n",
      "The police studied the stills from the security video . \n",
      "\n",
      "They have two houses here, and they have also bought a flat.\n",
      "They have two houses here , and they have also bought a flat . \n",
      "\n",
      "She was singing.\n",
      "She was singing . \n",
      "\n",
      "I read the newspaper.\n",
      "I read the newspaper . \n",
      "\n",
      "Beth allowed John to start shipping goods.\n",
      "Beth allowed John to start shipping goods . \n",
      "\n",
      "I do not like fish.\n",
      "I do not like fish . \n",
      "\n",
      "Do not come in front of me.\n",
      "Do not come in front of me . \n",
      "\n",
      "How many times can you face a lot of problems?\n",
      "How many times can you face a lot of problems ? \n",
      "\n",
      "The name is Bond, James Bond.\n",
      "The name is Bond , James Bond . \n",
      "\n",
      "Do not #hate me. I am just a @TA.\n",
      "Do not #hate me . I am just a @TA . \n",
      "\n",
      "Try-again. Unitl    you    succeed!\n",
      "Try - again . Unitl you succeed ! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run tokenizer.py test.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
